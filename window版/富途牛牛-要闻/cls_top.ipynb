{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313823c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import PyRSS2Gen\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88816ecc",
   "metadata": {},
   "source": [
    "# 获取页面的数据、静态、动态的 最终输出结果都是一个soup。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8190a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取  ajax网页 的 内容\n",
    "def get_soup_ajax(url):        \n",
    "    options = Options()#实例化一个chrome浏览器实例对象\n",
    "    options.add_argument(\"headless\") #不打开浏览器窗口 运行selenium\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    #options.add_argument(\"--remote-debugging-port=9222\")  # this\n",
    "\n",
    "    \n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15\"\n",
    "    options.add_argument('--user-agent=%s' % user_agent)\n",
    "    \n",
    "    driver = webdriver.Chrome( executable_path=chromedriver_path, options=options)#新建driver\n",
    "    driver.maximize_window() #最大化窗口\n",
    "    \n",
    "    driver.get(url) #获取页面内容\n",
    "    #time.sleep(5)    #等待5s，等待加载完成\n",
    "       \n",
    "    page_source = driver.page_source #获取页面源码数据  \n",
    "    soup = BeautifulSoup(page_source )  #用 BeautifulSoup解析\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb39caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取静态网页的内容\n",
    "def get_soup_static(url):\n",
    "    #请求头\n",
    "    headers = { \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",}\n",
    "    ret = requests.get(url,headers=headers)\n",
    "    ret.encoding = ret.apparent_encoding \n",
    "    #time.sleep(1)    \n",
    "    soup = BeautifulSoup(ret.text, 'html.parser') # 构建beautifulsoup实例\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c079f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 任何网页的内容，返回bs4的soup文件 \n",
    "def get_soup(url,is_ajax):\n",
    "    if is_ajax:\n",
    "        return get_soup_ajax(url)\n",
    "    else:\n",
    "        return get_soup_static(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6363c",
   "metadata": {},
   "source": [
    "# 2. 获取详情页的内容做为rss的description，默认使用详情页html的body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae043c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#该函数获取详情页的新闻内容\n",
    "def get_text(news_link):\n",
    "    headers = { \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",}\n",
    "    detialHtml=requests.get(news_link,headers=headers)\n",
    "    detialHtml.encoding = detialHtml.apparent_encoding \n",
    "    soup = BeautifulSoup(detialHtml.text, 'html.parser') # 构建beautifulsoup实例\n",
    "    #news_detail = soup.find(\"div\", class_=\"main\")#获取新闻内容详情\n",
    "    news_detail = soup.body #直接将详情页body做为新闻详情\n",
    "    time.sleep(3) #间隔时间防止反爬虫\n",
    "    news_detail =news_detail.text #要转为文本，不然后面会报错\n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068eacd",
   "metadata": {},
   "source": [
    "# 4.生成RSS的xml文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5f07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#该函数使用新闻的标题、链接、新闻内容，生成 PyRSS2Gen.RSS2函数所需要的参数 items\n",
    "def gen_rssitems(news_titles,news_links,news_details):\n",
    "    pubDate_now =datetime.datetime.now()\n",
    "    rssitems=[]\n",
    "    \n",
    "    for i in range(len(news_titles)):\n",
    "        rssitem = PyRSS2Gen.RSSItem(\n",
    "         title = news_titles[i],\n",
    "         link = news_links[i],\n",
    "\n",
    "         description = news_details[i],\n",
    "         #description = news_titles[i],\n",
    "         pubDate =pubDate_now)\n",
    "        \n",
    "        rssitems.append(rssitem)\n",
    "    return rssitems\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5a2f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#新闻标题、详情页、新闻内容链接 存入数组中\\nnews_links = []\\nnews_titles=[]\\nnews_details = []  \\n\\n##############下面的变量 要根据要制作rss的网页进行修改###############################\\nrss_path = \"./home/cls_top.xml\" #生成的RSS存放位置\\nurl = \\'https://www.cls.cn/depth?id=1000\\'  #要爬取的页面\\nchromedriver_path=\\'./chromedriver\\'   #chromedriver的存放位置\\nis_ajax = True    #是否为动态页面。对于静态网站：True时也能正常运行，但false会更快更省服务器资源。\\n\\nrss_title = \"财联社-头条\"  #rss的标题，会显示再rss阅读中\\nrss_description=\"财联社深度：重大政策事件及时分析解读。提供准确、快速、权威、专业的事件分析，涵盖新能源汽车、创业板、cpi、供给侧改革等板块，想了解更多财经新闻、股市行情请登陆财联社。\"  #rss的描述\\nsoup = get_soup(url,is_ajax) #网页的内容，返回bs4的soup文件 \\n\\n\\nnews_list= soup.find_all(\"div\", class_=\"clearfix b-c-e6e7ea subject-interest-list\") # 找到或精确 items位置  ，防止抓到其它版面内容  \\nnews =news_list[0]\\nnews_link=\"https://www.cls.cn\"+news.a.attrs[\\'href\\']\\nprint(news_link)\\nnews_title = news.div.div.a.get_text()\\nprint(news_title)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#调试代码备用\n",
    "'''\n",
    "#新闻标题、详情页、新闻内容链接 存入数组中\n",
    "news_links = []\n",
    "news_titles=[]\n",
    "news_details = []  \n",
    "\n",
    "##############下面的变量 要根据要制作rss的网页进行修改###############################\n",
    "rss_path = \"./home/cls_top.xml\" #生成的RSS存放位置\n",
    "url = 'https://www.cls.cn/depth?id=1000'  #要爬取的页面\n",
    "chromedriver_path='./chromedriver'   #chromedriver的存放位置\n",
    "is_ajax = True    #是否为动态页面。对于静态网站：True时也能正常运行，但false会更快更省服务器资源。\n",
    "\n",
    "rss_title = \"财联社-头条\"  #rss的标题，会显示再rss阅读中\n",
    "rss_description=\"财联社深度：重大政策事件及时分析解读。提供准确、快速、权威、专业的事件分析，涵盖新能源汽车、创业板、cpi、供给侧改革等板块，想了解更多财经新闻、股市行情请登陆财联社。\"  #rss的描述\n",
    "soup = get_soup(url,is_ajax) #网页的内容，返回bs4的soup文件 \n",
    "\n",
    "\n",
    "news_list= soup.find_all(\"div\", class_=\"clearfix b-c-e6e7ea subject-interest-list\") # 找到或精确 items位置  ，防止抓到其它版面内容  \n",
    "news =news_list[0]\n",
    "news_link=\"https://www.cls.cn\"+news.a.attrs['href']\n",
    "print(news_link)\n",
    "news_title = news.div.div.a.get_text()\n",
    "print(news_title)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bf1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catig\\AppData\\Local\\Temp\\ipykernel_3832\\2275567285.py:13: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome( executable_path=chromedriver_path, options=options)#新建driver\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #新闻标题、详情页、新闻内容链接 存入数组中\n",
    "    news_links = []\n",
    "    news_titles=[]\n",
    "    news_details = []  \n",
    "    \n",
    "    ##############下面的变量 要根据要制作rss的网页进行修改###############################\n",
    "    rss_path = \"./home/cls_top.xml\" #生成的RSS存放位置\n",
    "    url = 'https://www.cls.cn/depth?id=1000'  #要爬取的页面\n",
    "    chromedriver_path='./chromedriver'   #chromedriver的存放位置\n",
    "    is_ajax = True    #是否为动态页面。对于静态网站：True时也能正常运行，但false会更快更省服务器资源。\n",
    "    \n",
    "    rss_title = \"财联社-头条\"  #rss的标题，会显示再rss阅读中\n",
    "    rss_description=\"财联社深度：重大政策事件及时分析解读。提供准确、快速、权威、专业的事件分析，涵盖新能源汽车、创业板、cpi、供给侧改革等板块，想了解更多财经新闻、股市行情请登陆财联社。\"  #rss的描述\n",
    "    soup = get_soup(url,is_ajax) #网页的内容，返回bs4的soup文件 \n",
    "    \n",
    "    \n",
    "    news_list= soup.find_all(\"div\", class_=\"clearfix b-c-e6e7ea subject-interest-list\") # 找到或精确 items位置  ，防止抓到其它版面内容  \n",
    "  \n",
    "    for news in news_list:\n",
    "        news_link=\"https://www.cls.cn\"+news.a.attrs['href']   #详情页的url        \n",
    "        news_title = news.div.div.a.get_text()  #新闻的标题\n",
    "        #news_detail = get_text(news_link)  \n",
    "        news_detail = news_title  \n",
    "        \n",
    "\n",
    "        ##############上面的变量 要根据要制作rss的网页进行修改###############################\n",
    "        \n",
    "        news_links.append(news_link)\n",
    "        news_titles.append(news_title)\n",
    "        news_details.append(news_detail)\n",
    "    \n",
    "    rss = PyRSS2Gen.RSS2(\n",
    "    title = rss_title,\n",
    "    link = url,\n",
    "    description = rss_description,\n",
    "    lastBuildDate = datetime.datetime.now(),\n",
    "    items =gen_rssitems(news_titles,news_links,news_details))\n",
    "    rss.write_xml(open(rss_path, \"w\",encoding='UTF-16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7d117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
