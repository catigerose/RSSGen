{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313823c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import PyRSS2Gen\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "#from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1269ad",
   "metadata": {},
   "source": [
    "# 获取页面的数据、静态、动态的 最终输出结果都是一个soup。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6a09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取  ajax网页 的 内容\n",
    "def get_soup_ajax(url):        \n",
    "    options = Options()#实例化一个chrome浏览器实例对象\n",
    "    options.add_argument(\"headless\") #不打开浏览器窗口 运行selenium\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    #options.add_argument(\"--remote-debugging-port=9222\")  # this\n",
    "\n",
    "    \n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15\"\n",
    "    options.add_argument('--user-agent=%s' % user_agent)\n",
    "    \n",
    "    driver = webdriver.Chrome( executable_path=chromedriver_path, options=options)#新建driver\n",
    "    driver.maximize_window() #最大化窗口\n",
    "    \n",
    "    driver.get(url) #获取页面内容\n",
    "    #time.sleep(5)    #等待5s，等待加载完成\n",
    "       \n",
    "    page_source = driver.page_source #获取页面源码数据  \n",
    "    soup = BeautifulSoup(page_source )  #用 BeautifulSoup解析\n",
    "    driver.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2491cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取静态网页的内容\n",
    "def get_soup_static(url):\n",
    "    #请求头\n",
    "    headers = { \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",}\n",
    "    ret = requests.get(url,headers=headers)\n",
    "    ret.encoding = ret.apparent_encoding \n",
    "    #time.sleep(1)    \n",
    "    soup = BeautifulSoup(ret.text, 'html.parser') # 构建beautifulsoup实例\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51386a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 任何网页的内容，返回bs4的soup文件 \n",
    "def get_soup(url,is_ajax):\n",
    "    if is_ajax:\n",
    "        return get_soup_ajax(url)\n",
    "    else:\n",
    "        return get_soup_static(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccbdf5",
   "metadata": {},
   "source": [
    "# 2. 获取详情页的内容做为rss的description，默认使用详情页html的body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef82394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#该函数获取详情页的新闻内容\n",
    "def get_text(news_link):\n",
    "    headers = { \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",}\n",
    "    detialHtml=requests.get(news_link,headers=headers)\n",
    "    detialHtml.encoding = detialHtml.apparent_encoding \n",
    "    soup = BeautifulSoup(detialHtml.text, 'html.parser') # 构建beautifulsoup实例\n",
    "    #news_detail = soup.find(\"div\", class_=\"main\")#获取新闻内容详情\n",
    "    news_detail = soup.body #直接将详情页body做为新闻详情\n",
    "    time.sleep(3) #间隔时间防止反爬虫\n",
    "    news_detail =news_detail.text #要转为文本，不然后面会报错\n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068eacd",
   "metadata": {},
   "source": [
    "# 4.生成RSS的xml文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5f07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#该函数使用新闻的标题、链接、新闻内容，生成 PyRSS2Gen.RSS2函数所需要的参数 items\n",
    "def gen_rssitems(news_titles,news_links,news_details):\n",
    "    pubDate_now =datetime.datetime.now()\n",
    "    rssitems=[]\n",
    "    \n",
    "    for i in range(len(news_titles)):\n",
    "        rssitem = PyRSS2Gen.RSSItem(\n",
    "         title = news_titles[i],\n",
    "         link = news_links[i],\n",
    "\n",
    "         description = news_details[i],\n",
    "         #description = news_titles[i],\n",
    "         pubDate =pubDate_now)\n",
    "        \n",
    "        rssitems.append(rssitem)\n",
    "    return rssitems\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04092c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\catig\\AppData\\Local\\Temp\\ipykernel_12748\\2688309938.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome( executable_path=chromedriver_path, options=chrome_options)#新建driver\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #新闻标题、详情页、新闻内容链接 存入数组中\n",
    "    news_links = []\n",
    "    news_titles=[]\n",
    "    news_details = []  \n",
    "    \n",
    "    ##############下面的变量 要根据要制作rss的网页进行修改###############################\n",
    "    rss_path = \"./home/futunn.xml\" #生成的RSS存放位置\n",
    "    url = 'https://news.futunn.com/main?lang=zh-cn'  #要爬取的页面\n",
    "    chromedriver_path='./chromedriver'   #chromedriver的存放位置\n",
    "    is_ajax = True    #是否为动态页面。对于静态网站：True时也能正常运行，但false会更快更省服务器资源。\n",
    "    \n",
    "    rss_title = \"富途牛牛要闻\"  #rss的标题，会显示再rss阅读中\n",
    "    rss_description=\"财经新闻_最新全球财经资讯报道 - 富途牛牛\"  #rss的描述\n",
    "    soup = get_soup(url,is_ajax) #网页的内容，返回bs4的soup文件 \n",
    "    \n",
    "    \n",
    "    news_list= soup.find_all(\"li\", class_=\"news-li\") # 找到或精确 items位置  ，防止抓到其它版面内容  \n",
    "  \n",
    "    for news in news_list:\n",
    "        news_link = news.a.attrs['href']    #详情页的url        \n",
    "        news_title = news.a.div.h3.get_text()  #新闻的标题\n",
    "        news_detail = get_text(news_link)       \n",
    "        \n",
    "\n",
    "        ##############上面的变量 要根据要制作rss的网页进行修改###############################\n",
    "        \n",
    "        news_links.append(news_link)\n",
    "        news_titles.append(news_title)\n",
    "        news_details.append(news_detail)\n",
    "    \n",
    "    rss = PyRSS2Gen.RSS2(\n",
    "    title = rss_title,\n",
    "    link = url,\n",
    "    description = rss_description,\n",
    "    lastBuildDate = datetime.datetime.now(),\n",
    "    items =gen_rssitems(news_titles,news_links,news_details))\n",
    "    rss.write_xml(open(rss_path, \"w\",encoding='UTF-16'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
